{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam Dection_ RNN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRWpO2tNSxk+EhT2OTXxs7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlATdN9LQ-72"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JceUopcRzFq"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\", filename=\"spam.csv\")\n",
        "data = pd.read_csv('spam.csv',encoding='latin1')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4AlpC93Sd0l"
      },
      "source": [
        "print('TOTAL SAMPLE NUMBER :',len(data))\n",
        "\n",
        "data[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eHx47LSSjXS"
      },
      "source": [
        "del data['Unnamed: 2']\n",
        "del data['Unnamed: 3']\n",
        "del data['Unnamed: 4']\n",
        "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
        "data[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB4FMnQmSnZX"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlkuA11WTS5x"
      },
      "source": [
        "data.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgnvFJ7xTbgZ"
      },
      "source": [
        "data['v2'].nunique(), data['v1'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljQsSyW9UNNp"
      },
      "source": [
        "data['v1'].value_counts().plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6ggSLTvUa3k"
      },
      "source": [
        "print(\"ToTal Sample Number:\", len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd3VXNpOUfEb"
      },
      "source": [
        "data['v1'].value_counts().plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFWtgHLbUuYo"
      },
      "source": [
        "print(data.groupby('v1').size().reset_index(name='count'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2RtM0rjU19V"
      },
      "source": [
        "X_data = data['v2']\n",
        "Y_data = data['v1']\n",
        "print('Number of Mail {}'.format(len(X_data)))\n",
        "print('Number of Label {}'.format(len(y_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtn5hxtlVriA"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_data) # Line 5572 tokenizing\n",
        "sequences = tokenizer.texts_to_sequences(X_data) # Convert to Index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAX7mkeQWnWv"
      },
      "source": [
        "print(sequences[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOH8EV9MW5pn"
      },
      "source": [
        "word_to_index = tokenizer.word_index\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_sTrbb4YId7"
      },
      "source": [
        "word_to_counts = tokenizer.word_counts.items()\n",
        "print(word_to_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRQOExX2Zkkz"
      },
      "source": [
        "threshold = 2\n",
        "total_cnt = len(word_to_index) # Number of Word\n",
        "rare_cnt = 0 # Frequency of Threshold for small word counts\n",
        "total_freq = 0 # Training Data for total number of word for sum\n",
        "rare_freq = 0 # Sum of number for small counts \n",
        "\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # Frequency of Words less than threshold\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('Frequency of numbers %së²ˆ Under rare value: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"Rate of rare words of vocabulary:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"Percentage of rare words of overall frequency:\", (rare_freq / total_freq)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iinmn2Lbxyd"
      },
      "source": [
        "vocab_size = len(word_to_index) + 1\n",
        "print('Size of Words: {}'.format((vocab_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WZK4rVedbll"
      },
      "source": [
        "n_of_train = int(len(sequences) * 0.8)\n",
        "n_of_test = int(len(sequences) - n_of_train)\n",
        "print('number of training data:', n_of_train)\n",
        "print('number of test data:', n_of_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTR7Q3iweS82"
      },
      "source": [
        "X_data = sequences\n",
        "print('Maximum length of Mail : %d' % max(len(l) for l in X_data))\n",
        "print('Average length of Mail : %f' % (sum(map(len, X_data))/len(X_data)))\n",
        "plt.hist([len(s) for s in X_data], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZDOzdwYe3GN"
      },
      "source": [
        "max_len = 189\n",
        "# Length of Total Dataset is fit into max_len\n",
        "data = pad_sequences(X_data, maxlen = max_len)\n",
        "print(\"Size of Training Data(Shape)\", data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5oMmsr-fp9S"
      },
      "source": [
        "X_test = data[n_of_train:] # In X_Data, store just 1034 dataset\n",
        "y_test = np.array(y_data[n_of_train:]) # In y_data, we have to add 1034 \n",
        "\n",
        "X_train = data[:n_of_train] # In X_train, let's load 4135 dataset\n",
        "y_train = np.array(y_data[:n_of_train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAoklmflgjPP"
      },
      "source": [
        "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhGOd3-HhaEo"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 32)) # vector size is 32\n",
        "model.add(SimpleRNN(32)) # hidden size is 32\n",
        "model.add(Dense(1, activation ='sigmoid'))\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "history = model.fit(X_train, y_train, epochs = 4, batch_size = 64, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyL2SHhvjUYt"
      },
      "source": [
        "print(\"\\n Test Accuracy %.4f\" %(model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD3LnLKsjku1"
      },
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "plt.plot(epochs, history.history['loss'])\n",
        "plt.plot(epochs, history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}